{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import kuzu\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import shutil\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 entities and 27714 transactions (after typologies)\n",
      "typology\n",
      "benign        25000\n",
      "smurfing       1424\n",
      "money_mule      798\n",
      "cls             492\n",
      "Name: count, dtype: int64\n",
      "count     27714.000000\n",
      "mean      47739.861370\n",
      "std       47348.665181\n",
      "min          81.681115\n",
      "25%       15102.939172\n",
      "50%       33895.386598\n",
      "75%       64331.302178\n",
      "max      524429.154457\n",
      "Name: amount, dtype: float64\n",
      "is_cross_border  ml_flag  flagged_receiver  high_risk_jurisdiction\n",
      "True             0        False             False                     12443\n",
      "False            0        False             False                      7625\n",
      "True             0        False             True                       3260\n",
      "                          True              True                        820\n",
      "                 1        False             True                        713\n",
      "False            1        True              False                       475\n",
      "True             1        False             False                       474\n",
      "                          True              True                        391\n",
      "False            0        False             True                        381\n",
      "True             0        True              False                       239\n",
      "                 1        True              False                       223\n",
      "False            1        False             False                       170\n",
      "                 0        True              False                       146\n",
      "                 1        False             True                        143\n",
      "                          True              True                        125\n",
      "                 0        True              True                         86\n",
      "Name: count, dtype: int64\n",
      "       kyc_risk_score  dormancy_period\n",
      "count     5000.000000       5000.00000\n",
      "mean         0.161390        184.23860\n",
      "std          0.119509        107.46707\n",
      "min          0.000053          0.00000\n",
      "25%          0.076511         89.00000\n",
      "50%          0.152034        186.00000\n",
      "75%          0.226321        280.00000\n",
      "max          0.898375        364.00000\n",
      "Proportion of ml_flag = 1: 0.0979\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_data(num_entities=5000, num_transactions=25000):\n",
    "    np.random.seed(42)\n",
    "    # Step 1: Benign data\n",
    "    countries = np.random.choice(['US', 'EU', 'ASIA', 'HIGH_RISK'], num_entities, p=[0.4, 0.3, 0.2, 0.1])\n",
    "    entities = pd.DataFrame({\n",
    "        'entity_id': [f'E{i:04d}' for i in range(num_entities)],\n",
    "        'profile_type': np.random.choice(['individual_low', 'individual_high', 'business_small', 'business_large'], num_entities, p=[0.4, 0.1, 0.3, 0.2]),\n",
    "        'country': countries,\n",
    "        'agent_id': np.random.randint(1, 100, num_entities),\n",
    "        'kyc_risk_score': np.random.uniform(0, 0.3, num_entities),\n",
    "        'dormancy_period': np.random.randint(0, 365, num_entities)\n",
    "    })\n",
    "    G = nx.watts_strogatz_graph(num_entities, k=10, p=0.3, seed=42)  # Watts-Strogatz for cycles\n",
    "    edges = [(f'E{u:04d}', f'E{v:04d}') for u, v in G.edges()]\n",
    "    transactions = pd.DataFrame({\n",
    "        'sender_id': [edges[i % len(edges)][0] for i in range(num_transactions)],\n",
    "        'receiver_id': [edges[i % len(edges)][1] for i in range(num_transactions)],\n",
    "        'amount': np.random.exponential(50000, num_transactions).clip(100, 1500000),\n",
    "        'timestamp': np.random.randint(0, 30, num_transactions),\n",
    "        'ml_flag': np.zeros(num_transactions, dtype=int),\n",
    "        'typology': ['benign'] * num_transactions\n",
    "    })\n",
    "    transactions['is_cross_border'] = transactions.apply(lambda row: entities.loc[entities['entity_id'] == row['sender_id'], 'country'].values[0] != entities.loc[entities['entity_id'] == row['receiver_id'], 'country'].values[0], axis=1)\n",
    "    transactions['high_risk_jurisdiction'] = transactions.apply(lambda row: 'HIGH_RISK' in [entities.loc[entities['entity_id'] == row['sender_id'], 'country'].values[0], entities.loc[entities['entity_id'] == row['receiver_id'], 'country'].values[0]], axis=1)\n",
    "    transactions['flagged_receiver'] = np.random.choice([True, False], num_transactions, p=[0.02, 0.98])\n",
    "\n",
    "    # Step 2: Inject typologies (2% suspicious entities)\n",
    "    num_suspicious = int(0.02 * num_entities)\n",
    "    suspicious_ids = np.random.choice(entities['entity_id'], num_suspicious, replace=False)\n",
    "    for sid in suspicious_ids:\n",
    "        entities.loc[entities['entity_id'] == sid, 'kyc_risk_score'] = np.random.uniform(0.6, 0.9)\n",
    "        entities.loc[entities['entity_id'] == sid, 'dormancy_period'] = np.random.randint(200, 365)\n",
    "        ts = np.random.randint(0, 3)\n",
    "        \n",
    "        # Typology 1: Smurfing\n",
    "        large_amount = np.random.uniform(200000, 800000)\n",
    "        num_smurfs = np.random.randint(10, 20)\n",
    "        small_amounts = np.full(num_smurfs, large_amount / num_smurfs)\n",
    "        receivers = np.random.choice(entities['entity_id'], num_smurfs, replace=False)\n",
    "        smurf_tx = pd.DataFrame({\n",
    "            'sender_id': [sid] * num_smurfs,\n",
    "            'receiver_id': receivers,\n",
    "            'amount': small_amounts,\n",
    "            'timestamp': [ts] * num_smurfs,\n",
    "            'ml_flag': 1,\n",
    "            'is_cross_border': np.random.choice([True, False], num_smurfs, p=[0.7, 0.3]),\n",
    "            'high_risk_jurisdiction': np.random.choice([True, False], num_smurfs, p=[0.5, 0.5]),\n",
    "            'flagged_receiver': np.random.choice([True, False], num_smurfs, p=[0.3, 0.7]),\n",
    "            'typology': ['smurfing'] * num_smurfs\n",
    "        })\n",
    "        transactions = pd.concat([transactions, smurf_tx], ignore_index=True)\n",
    "        \n",
    "        # Typology 2: Money Mules\n",
    "        num_mules = np.random.randint(3, 6)\n",
    "        mules = np.random.choice(entities['entity_id'], num_mules, replace=False)\n",
    "        mule_amount = np.random.uniform(5000, 50000)\n",
    "        for mule in mules:\n",
    "            final = np.random.choice(entities[entities['country'] == 'HIGH_RISK']['entity_id'], 1)[0] if np.random.rand() > 0.5 else sid\n",
    "            mule_tx = pd.DataFrame({\n",
    "                'sender_id': [sid, mule],\n",
    "                'receiver_id': [mule, final],\n",
    "                'amount': [mule_amount] * 2,\n",
    "                'timestamp': [ts, ts + 1],\n",
    "                'ml_flag': 1,\n",
    "                'is_cross_border': [False, True],\n",
    "                'high_risk_jurisdiction': [False, True],\n",
    "                'flagged_receiver': [True, False],\n",
    "                'typology': ['money_mule'] * 2\n",
    "            })\n",
    "            transactions = pd.concat([transactions, mule_tx], ignore_index=True)\n",
    "        \n",
    "        # Typology 3: CLS\n",
    "        layers = np.random.randint(4, 7)\n",
    "        chain = [sid] + list(np.random.choice(entities['entity_id'], layers - 1, replace=False)) + [sid if np.random.rand() > 0.5 else np.random.choice(entities['entity_id'])]\n",
    "        layer_amount = np.random.uniform(10000, 100000)\n",
    "        for j in range(len(chain) - 1):\n",
    "            cls_tx = pd.DataFrame({\n",
    "                'sender_id': [chain[j]],\n",
    "                'receiver_id': [chain[j+1]],\n",
    "                'amount': [layer_amount],\n",
    "                'timestamp': [ts + j],\n",
    "                'ml_flag': 1,\n",
    "                'is_cross_border': np.random.choice([True, False], 1, p=[0.8, 0.2]),\n",
    "                'high_risk_jurisdiction': np.random.choice([True, False], 1, p=[0.6, 0.4]),\n",
    "                'flagged_receiver': np.random.choice([True, False], 1, p=[0.4, 0.6]),\n",
    "                'typology': ['cls']\n",
    "            })\n",
    "            transactions = pd.concat([transactions, cls_tx], ignore_index=True)\n",
    "\n",
    "    # Step 3: Refine realism\n",
    "    noise_idx = np.random.choice(transactions[transactions['typology'] == 'benign'].index, int(0.1 * num_transactions))\n",
    "    transactions.loc[noise_idx, 'amount'] *= np.random.uniform(0.8, 1.2)\n",
    "    transactions.loc[noise_idx, 'is_cross_border'] = np.random.choice([True, False], len(noise_idx), p=[0.4, 0.6])\n",
    "    high_risk_noise = transactions[transactions['high_risk_jurisdiction']]\n",
    "    if not high_risk_noise.empty:\n",
    "        high_risk_noise_idx = np.random.choice(high_risk_noise.index, int(0.2 * len(high_risk_noise)))\n",
    "        transactions.loc[high_risk_noise_idx, 'flagged_receiver'] = True\n",
    "\n",
    "    entities.to_csv('entities.csv', index=False)\n",
    "    transactions.to_csv('transactions.csv', index=False)\n",
    "    print(f\"Generated {num_entities} entities and {len(transactions)} transactions (after typologies)\")\n",
    "    print(transactions['typology'].value_counts())\n",
    "    print(transactions['amount'].describe())\n",
    "    print(transactions[['is_cross_border', 'ml_flag', 'flagged_receiver', 'high_risk_jurisdiction']].value_counts())\n",
    "    print(entities[['kyc_risk_score', 'dormancy_period']].describe())\n",
    "    print(f\"Proportion of ml_flag = 1: {transactions['ml_flag'].mean():.4f}\")\n",
    "    return entities, transactions\n",
    "\n",
    "entities, transactions = generate_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 entities and 27714 transactions\n",
      "Warning: 10 duplicate edges found; keeping first occurrence\n",
      "After dropping duplicates, 27709 transactions remain\n",
      "Graph built with 5000 nodes and 27709 edges\n",
      "ml_flag_score stats: mean=0.0425, non-zero=818\n",
      "Merged data_df shape: (5000, 29)\n",
      "NaN counts:\n",
      " entity_id                 0\n",
      "degree                    0\n",
      "small_tx_count            0\n",
      "avg_amount                0\n",
      "amount_variance           0\n",
      "amount_skewness           0\n",
      "high_value_ratio          0\n",
      "cycle_score               0\n",
      "clustering_coeff          0\n",
      "tx_freq_variance          0\n",
      "tx_velocity               0\n",
      "burstiness                0\n",
      "temporal_concentration    0\n",
      "kyc_risk_score            0\n",
      "dormancy_period           0\n",
      "cross_border_amount       0\n",
      "in_degree_x               0\n",
      "tx_frequency              0\n",
      "directionality_ratio      0\n",
      "round_trip_count          0\n",
      "in_degree_y               0\n",
      "profile_type              0\n",
      "agent_id                  0\n",
      "country_ASIA              0\n",
      "country_EU                0\n",
      "country_HIGH_RISK         0\n",
      "country_US                0\n",
      "ml_flag_score             0\n",
      "is_anomaly                0\n",
      "dtype: int64\n",
      "Data types:\n",
      " entity_id                  object\n",
      "degree                      int64\n",
      "small_tx_count              int64\n",
      "avg_amount                float64\n",
      "amount_variance           float64\n",
      "amount_skewness           float64\n",
      "high_value_ratio          float64\n",
      "cycle_score               float64\n",
      "clustering_coeff          float64\n",
      "tx_freq_variance          float64\n",
      "tx_velocity               float64\n",
      "burstiness                float64\n",
      "temporal_concentration      int64\n",
      "kyc_risk_score            float64\n",
      "dormancy_period             int64\n",
      "cross_border_amount       float64\n",
      "in_degree_x                 int64\n",
      "tx_frequency                int64\n",
      "directionality_ratio      float64\n",
      "round_trip_count            int64\n",
      "in_degree_y                 int64\n",
      "profile_type               object\n",
      "agent_id                    int64\n",
      "country_ASIA                 bool\n",
      "country_EU                   bool\n",
      "country_HIGH_RISK            bool\n",
      "country_US                   bool\n",
      "ml_flag_score             float64\n",
      "is_anomaly                  int64\n",
      "dtype: object\n",
      "Feature stats:\n",
      "             degree  small_tx_count     avg_amount  amount_variance  \\\n",
      "count  5000.000000     5000.000000    5000.000000     5.000000e+03   \n",
      "mean      5.541800        0.095400   48755.849932     2.331036e+09   \n",
      "std       3.132829        0.308412   22872.017588     3.182987e+09   \n",
      "min       0.000000        0.000000       0.000000     0.000000e+00   \n",
      "25%       4.000000        0.000000   33182.569471     5.701476e+08   \n",
      "50%       5.000000        0.000000   45103.073161     1.305819e+09   \n",
      "75%       6.000000        0.000000   60374.291354     2.798116e+09   \n",
      "max      31.000000        3.000000  254844.399765     4.711052e+10   \n",
      "\n",
      "       amount_skewness  high_value_ratio  cycle_score  clustering_coeff  \\\n",
      "count      5000.000000       5000.000000  5000.000000       5000.000000   \n",
      "mean          0.411063          0.125525     1.903491          0.205335   \n",
      "std           0.510927          0.155380     0.048862          0.105255   \n",
      "min          -1.990854          0.000000     0.000000          0.000000   \n",
      "25%           0.050736          0.000000     1.883800          0.133333   \n",
      "50%           0.381856          0.000000     1.903453          0.194444   \n",
      "75%           0.722293          0.200000     1.924321          0.266667   \n",
      "max           3.878899          1.000000     1.999999          0.761905   \n",
      "\n",
      "       tx_freq_variance   tx_velocity  ...  dormancy_period  \\\n",
      "count       5000.000000  5.000000e+03  ...       5000.00000   \n",
      "mean           1.617278  8.000030e+04  ...        184.23860   \n",
      "std            2.683003  1.016763e+06  ...        107.46707   \n",
      "min            0.458200  0.000000e+00  ...          0.00000   \n",
      "25%            0.541800  2.105263e-01  ...         89.00000   \n",
      "50%            1.458200  2.631579e-01  ...        186.00000   \n",
      "75%            1.541800  3.333333e-01  ...        280.00000   \n",
      "max           25.458200  2.000000e+07  ...        364.00000   \n",
      "\n",
      "       cross_border_amount  in_degree_x  tx_frequency  directionality_ratio  \\\n",
      "count          5000.000000  5000.000000   5000.000000           5000.000000   \n",
      "mean             11.629865     5.541800      5.541800              0.504352   \n",
      "std               1.672176     1.946641      3.132829              0.141229   \n",
      "min               0.000000     0.000000      0.000000              0.000000   \n",
      "25%              11.342662     4.000000      4.000000              0.400000   \n",
      "50%              11.938876     5.000000      5.000000              0.500000   \n",
      "75%              12.396172     7.000000      6.000000              0.600000   \n",
      "max              13.917339    14.000000     31.000000              1.000000   \n",
      "\n",
      "       round_trip_count  in_degree_y     agent_id  ml_flag_score  is_anomaly  \n",
      "count       5000.000000  5000.000000  5000.000000    5000.000000  5000.00000  \n",
      "mean           0.079200     5.541800    49.382000       0.042512     0.16280  \n",
      "std            0.373839     1.946641    28.521325       0.127628     0.36922  \n",
      "min            0.000000     0.000000     1.000000       0.000000     0.00000  \n",
      "25%            0.000000     4.000000    24.000000       0.000000     0.00000  \n",
      "50%            0.000000     5.000000    50.000000       0.000000     0.00000  \n",
      "75%            0.000000     7.000000    74.000000       0.000000     0.00000  \n",
      "max            5.000000    14.000000    99.000000       0.944444     1.00000  \n",
      "\n",
      "[8 rows x 23 columns]\n",
      "Feature correlations with ml_flag_score:\n",
      " degree                    0.300678\n",
      "small_tx_count            0.027757\n",
      "avg_amount               -0.032375\n",
      "amount_variance          -0.050810\n",
      "amount_skewness           0.113326\n",
      "high_value_ratio         -0.063062\n",
      "cycle_score              -0.154351\n",
      "clustering_coeff         -0.309066\n",
      "tx_freq_variance          0.049069\n",
      "tx_velocity               0.122425\n",
      "burstiness                0.071604\n",
      "temporal_concentration   -0.047248\n",
      "kyc_risk_score            0.093974\n",
      "dormancy_period           0.049011\n",
      "cross_border_amount       0.196131\n",
      "in_degree_x               0.246298\n",
      "tx_frequency              0.300678\n",
      "directionality_ratio     -0.054506\n",
      "round_trip_count          0.570060\n",
      "in_degree_y               0.246298\n",
      "Name: ml_flag_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Load data\n",
    "entities = pd.read_csv('entities.csv')\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "\n",
    "# Validate input data\n",
    "if entities.empty or transactions.empty:\n",
    "    raise ValueError(\"Entities or transactions DataFrame is empty.\")\n",
    "print(f\"Loaded {len(entities)} entities and {len(transactions)} transactions\")\n",
    "\n",
    "# Check for invalid or missing IDs in transactions\n",
    "invalid_transactions = transactions[transactions['sender_id'].isna() | transactions['receiver_id'].isna()]\n",
    "if not invalid_transactions.empty:\n",
    "    print(f\"Warning: {len(invalid_transactions)} transactions with missing sender_id or receiver_id\")\n",
    "    transactions = transactions.dropna(subset=['sender_id', 'receiver_id'])\n",
    "    print(f\"After dropping invalid transactions, {len(transactions)} transactions remain\")\n",
    "\n",
    "# Check for duplicate transactions\n",
    "transactions['edge'] = transactions['sender_id'] + '->' + transactions['receiver_id']\n",
    "duplicate_edges = transactions[transactions.duplicated(subset=['edge'], keep=False)]\n",
    "if not duplicate_edges.empty:\n",
    "    print(f\"Warning: {len(duplicate_edges)} duplicate edges found; keeping first occurrence\")\n",
    "    transactions = transactions.drop_duplicates(subset=['edge'], keep='first')\n",
    "    print(f\"After dropping duplicates, {len(transactions)} transactions remain\")\n",
    "\n",
    "# Ensure all transaction nodes are in entities\n",
    "all_entity_ids = set(entities['entity_id']).union(transactions['sender_id'], transactions['receiver_id'])\n",
    "missing_entities = all_entity_ids - set(entities['entity_id'])\n",
    "if missing_entities:\n",
    "    print(f\"Adding {len(missing_entities)} missing entities to entities DataFrame\")\n",
    "    missing_df = pd.DataFrame({\n",
    "        'entity_id': list(missing_entities),\n",
    "        'profile_type': 'UNKNOWN',\n",
    "        'country': 'UNKNOWN',\n",
    "        'agent_id': -1,\n",
    "        'kyc_risk_score': 0,\n",
    "        'dormancy_period': 0\n",
    "    })\n",
    "    entities = pd.concat([entities, missing_df], ignore_index=True)\n",
    "    entities.to_csv('entities.csv', index=False)\n",
    "\n",
    "# Build directed graph\n",
    "G = nx.DiGraph()\n",
    "for _, row in entities.iterrows():\n",
    "    G.add_node(row['entity_id'], **row.to_dict())\n",
    "for _, row in transactions.iterrows():\n",
    "    G.add_edge(row['sender_id'], row['receiver_id'], **row.to_dict())\n",
    "\n",
    "# Verify graph\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "print(f\"Graph built with {num_nodes} nodes and {num_edges} edges\")\n",
    "if num_nodes != len(entities):\n",
    "    raise ValueError(f\"Graph has {num_nodes} nodes, but {len(entities)} entities expected\")\n",
    "if num_edges != len(transactions):\n",
    "    print(f\"Error: Graph has {num_edges} edges, but {len(transactions)} transactions expected\")\n",
    "    # Debug mismatched transactions\n",
    "    graph_edges = set((u, v) for u, v in G.edges())\n",
    "    transaction_edges = set((row['sender_id'], row['receiver_id']) for _, row in transactions.iterrows())\n",
    "    missing_edges = transaction_edges - graph_edges\n",
    "    print(f\"Missing edges: {missing_edges}\")\n",
    "    raise ValueError(f\"Graph edge mismatch; {len(missing_edges)} edges not added\")\n",
    "\n",
    "# Feature extraction\n",
    "degree_dict = dict(G.out_degree())\n",
    "degree_df = pd.DataFrame({'entity_id': list(degree_dict.keys()), 'degree': list(degree_dict.values())}).fillna(0)\n",
    "in_degree_dict = dict(G.in_degree())\n",
    "in_degree_df = pd.DataFrame({'entity_id': list(in_degree_dict.keys()), 'in_degree': list(in_degree_dict.values())}).fillna(0)\n",
    "small_tx = {}\n",
    "for node in G.nodes():\n",
    "    small_tx[node] = sum(1 for _, _, data in G.out_edges(node, data=True) if data.get('amount', 0) < 1000)\n",
    "small_tx_df = pd.DataFrame({'entity_id': list(small_tx.keys()), 'small_tx_count': list(small_tx.values())}).fillna(0)\n",
    "undirected_G = G.to_undirected()\n",
    "clustering_dict = nx.clustering(undirected_G)\n",
    "cluster_df = pd.DataFrame({'entity_id': list(clustering_dict.keys()), 'clustering_coeff': list(clustering_dict.values())}).fillna(0)\n",
    "tx_count = {node: G.out_degree(node) for node in G.nodes()}\n",
    "tx_count_df = pd.DataFrame({'entity_id': list(tx_count.keys()), 'tx_count': list(tx_count.values())}).fillna(0)\n",
    "mean_tx_count = tx_count_df['tx_count'].mean()\n",
    "tx_count_df['tx_freq_variance'] = np.sqrt((tx_count_df['tx_count'] - mean_tx_count) ** 2).fillna(0)\n",
    "tx_freq_df = tx_count_df[['entity_id', 'tx_freq_variance']]\n",
    "amount_data = {}\n",
    "for node in G.nodes():\n",
    "    amounts = [data.get('amount', 0) for _, _, data in G.out_edges(node, data=True)]\n",
    "    if len(amounts) > 0:\n",
    "        avg = np.mean(amounts)\n",
    "        var = np.var(amounts, ddof=1) if len(amounts) > 1 else 0\n",
    "        skew = (((np.array(amounts) - avg) ** 3).mean()) / (np.std(amounts, ddof=1) ** 3 + 1e-7) if len(amounts) > 1 else 0\n",
    "        high_ratio = sum(a > 100000 for a in amounts) / len(amounts)\n",
    "        cycle_score = np.log1p(sum(a**2 for a in amounts) / len(amounts)) / np.log1p(max(amounts)) if max(amounts) > 0 else 0\n",
    "    else:\n",
    "        avg = var = skew = high_ratio = cycle_score = 0\n",
    "    amount_data[node] = {'avg_amount': avg, 'amount_variance': var, 'amount_skewness': skew, 'high_value_ratio': high_ratio, 'cycle_score': cycle_score}\n",
    "var_skew_df = pd.DataFrame.from_dict(amount_data, orient='index').reset_index().rename(columns={'index': 'entity_id'}).fillna(0)\n",
    "time_data = {}\n",
    "for node in G.nodes():\n",
    "    timestamps = [data.get('timestamp', 0) for _, _, data in G.out_edges(node, data=True)]\n",
    "    if len(timestamps) > 0:\n",
    "        min_t, max_t = min(timestamps), max(timestamps)\n",
    "        velocity = len(timestamps) / (max_t - min_t + 1e-7)\n",
    "        burst = np.var(timestamps, ddof=1) * 10 if len(timestamps) > 1 else 0\n",
    "        conc = 1 if (max_t - min_t < 3) else 0\n",
    "    else:\n",
    "        velocity = burst = conc = 0\n",
    "    time_data[node] = {'tx_velocity': velocity, 'burstiness': burst, 'temporal_concentration': conc}\n",
    "time_agg_df = pd.DataFrame.from_dict(time_data, orient='index').reset_index().rename(columns={'index': 'entity_id'}).fillna(0)\n",
    "features = {}\n",
    "for node in G.nodes():\n",
    "    node_data = G.nodes[node]\n",
    "    out_edges = list(G.out_edges(node, data=True))\n",
    "    in_edges = list(G.in_edges(node, data=True))\n",
    "    cross_border_amount = np.log1p(sum(data.get('amount', 0) for _, _, data in out_edges if data.get('is_cross_border', False)))\n",
    "    tx_frequency = len(out_edges)\n",
    "    directionality_ratio = len(in_edges) / (len(in_edges) + tx_frequency + 1e-7)\n",
    "    features[node] = {\n",
    "        'kyc_risk_score': node_data.get('kyc_risk_score', 0),\n",
    "        'dormancy_period': node_data.get('dormancy_period', 0),\n",
    "        'cross_border_amount': cross_border_amount,\n",
    "        'in_degree': len(in_edges),\n",
    "        'tx_frequency': tx_frequency,\n",
    "        'directionality_ratio': directionality_ratio\n",
    "    }\n",
    "features_df = pd.DataFrame.from_dict(features, orient='index').reset_index().rename(columns={'index': 'entity_id'}).fillna(0)\n",
    "round_trip = {}\n",
    "for node in G.nodes():\n",
    "    count = 0\n",
    "    for neighbor in G.neighbors(node):\n",
    "        for next_neighbor in G.neighbors(neighbor):\n",
    "            if next_neighbor == node and neighbor != node:\n",
    "                count += 1\n",
    "    round_trip[node] = count\n",
    "round_trip_df = pd.DataFrame({'entity_id': list(round_trip.keys()), 'round_trip_count': list(round_trip.values())}).fillna(0)\n",
    "\n",
    "# Merge features\n",
    "all_entities = pd.DataFrame({'entity_id': entities['entity_id']})\n",
    "data_df = all_entities.merge(degree_df, on='entity_id', how='left')\\\n",
    "                     .merge(small_tx_df, on='entity_id', how='left')\\\n",
    "                     .merge(var_skew_df, on='entity_id', how='left')\\\n",
    "                     .merge(cluster_df, on='entity_id', how='left')\\\n",
    "                     .merge(tx_freq_df, on='entity_id', how='left')\\\n",
    "                     .merge(time_agg_df, on='entity_id', how='left')\\\n",
    "                     .merge(features_df, on='entity_id', how='left')\\\n",
    "                     .merge(round_trip_df, on='entity_id', how='left')\\\n",
    "                     .merge(in_degree_df, on='entity_id', how='left')\\\n",
    "                     .merge(entities[['entity_id', 'country', 'profile_type', 'agent_id']], on='entity_id', how='left')\n",
    "\n",
    "# One-hot encode 'country'\n",
    "data_df = pd.get_dummies(data_df, columns=['country'], prefix='country')\n",
    "\n",
    "# Fill missing values\n",
    "data_df.fillna({\n",
    "    'degree': 0, 'small_tx_count': 0, 'avg_amount': 0, 'amount_variance': 0,\n",
    "    'amount_skewness': 0, 'high_value_ratio': 0, 'cycle_score': 0, 'clustering_coeff': 0,\n",
    "    'tx_freq_variance': 0, 'tx_velocity': 0, 'burstiness': 0,\n",
    "    'temporal_concentration': 0, 'kyc_risk_score': 0, 'dormancy_period': 0,\n",
    "    'cross_border_amount': 0, 'in_degree': 0, 'tx_frequency': 0,\n",
    "    'directionality_ratio': 0, 'round_trip_count': 0,\n",
    "    'profile_type': 'UNKNOWN', 'agent_id': -1\n",
    "}, inplace=True)\n",
    "\n",
    "# Compute ml_flag_score for anomaly labeling\n",
    "ml_flag_dict = {}\n",
    "for node in entities['entity_id']:\n",
    "    ml_flags = [data.get('ml_flag', 0) for u, v, data in G.out_edges(node, data=True)]\n",
    "    ml_flag_dict[node] = np.mean(ml_flags) if ml_flags else 0\n",
    "ml_flag_df = pd.DataFrame({'entity_id': list(ml_flag_dict.keys()), 'ml_flag_score': list(ml_flag_dict.values())}).fillna(0)\n",
    "data_df = data_df.merge(ml_flag_df, on='entity_id', how='left').fillna({'ml_flag_score': 0})\n",
    "\n",
    "# Debug merge\n",
    "if 'ml_flag_score' not in data_df.columns:\n",
    "    raise ValueError(\"ml_flag_score column missing after merge\")\n",
    "print(f\"ml_flag_score stats: mean={data_df['ml_flag_score'].mean():.4f}, non-zero={data_df['ml_flag_score'].gt(0).sum()}\")\n",
    "\n",
    "# Define anomaly labels\n",
    "data_df['is_anomaly'] = (data_df['ml_flag_score'] > 0).astype(int)\n",
    "if data_df['is_anomaly'].sum() > 1:\n",
    "    fpr, tpr, thresholds = roc_curve(data_df['is_anomaly'], data_df['ml_flag_score'])\n",
    "    anomaly_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    data_df['is_anomaly'] = (data_df['ml_flag_score'] > anomaly_threshold).astype(int)\n",
    "else:\n",
    "    print(\"Warning: Too few positive samples; using 95th percentile threshold\")\n",
    "    anomaly_threshold = np.percentile(data_df['ml_flag_score'], 95)\n",
    "    data_df['is_anomaly'] = (data_df['ml_flag_score'] > anomaly_threshold).astype(int)\n",
    "\n",
    "# Generate adjacency matrix\n",
    "adj = nx.adjacency_matrix(G, nodelist=data_df['entity_id'].tolist(), weight='amount')\n",
    "adj = sp.csr_matrix(adj)\n",
    "\n",
    "# Validate features\n",
    "print(\"Merged data_df shape:\", data_df.shape)\n",
    "print(\"NaN counts:\\n\", data_df.isna().sum())\n",
    "print(\"Data types:\\n\", data_df.dtypes)\n",
    "print(\"Feature stats:\\n\", data_df.describe())\n",
    "numeric_cols = [col for col in data_df.columns if col not in ['entity_id', 'is_anomaly', 'profile_type', 'agent_id']]\n",
    "if not np.all(np.isfinite(data_df[numeric_cols].select_dtypes(include=[np.number]))):\n",
    "    print(\"Warning: Non-finite values detected; replacing with 0\")\n",
    "    data_df[numeric_cols] = data_df[numeric_cols].fillna(0)\n",
    "\n",
    "# Check for leakage with robust correlation\n",
    "non_onehot_cols = [col for col in numeric_cols if not col.startswith('country_') and col != 'ml_flag_score']\n",
    "if non_onehot_cols:\n",
    "    correlations = data_df[non_onehot_cols + ['ml_flag_score']].corr(method='spearman')['ml_flag_score'].drop('ml_flag_score', errors='ignore').fillna(0)\n",
    "    print(\"Feature correlations with ml_flag_score:\\n\", correlations)\n",
    "else:\n",
    "    print(\"Warning: No non-onehot numeric columns for correlation analysis\")\n",
    "\n",
    "# Save for debugging\n",
    "data_df.to_csv('merged_features.csv', index=False)\n",
    "sp.save_npz('adjacency_matrix.npz', adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n",
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 20, Loss: 0.6865\n",
      "Epoch 40, Loss: 0.6799\n",
      "Epoch 60, Loss: 0.6735\n",
      "Epoch 80, Loss: 0.6672\n",
      "Epoch 100, Loss: 0.6611\n",
      "Epoch 120, Loss: 0.6551\n",
      "Epoch 140, Loss: 0.6493\n",
      "Epoch 160, Loss: 0.6435\n",
      "Epoch 180, Loss: 0.6380\n",
      "Epoch 200, Loss: 0.6325\n",
      "Epoch 220, Loss: 0.6272\n",
      "Epoch 240, Loss: 0.6220\n",
      "Epoch 260, Loss: 0.6169\n",
      "Epoch 280, Loss: 0.6120\n",
      "Fold 1 Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUPRC: 0.5786, AUC: 0.5011\n",
      "Validation Anomaly Scores (first 10): [0.22959778 0.22959778 0.22959778 0.22959778 0.22959778 0.22959778\n",
      " 0.22959778 0.22959778 0.22959778 0.22959778]\n",
      "Training fold 2/5\n",
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 20, Loss: 0.6863\n",
      "Epoch 40, Loss: 0.6792\n",
      "Epoch 60, Loss: 0.6722\n",
      "Epoch 80, Loss: 0.6655\n",
      "Epoch 100, Loss: 0.6591\n",
      "Epoch 120, Loss: 0.6528\n",
      "Epoch 140, Loss: 0.6468\n",
      "Epoch 160, Loss: 0.6409\n",
      "Epoch 180, Loss: 0.6353\n",
      "Epoch 200, Loss: 0.6297\n",
      "Epoch 220, Loss: 0.6244\n",
      "Epoch 240, Loss: 0.6192\n",
      "Epoch 260, Loss: 0.6142\n",
      "Epoch 280, Loss: 0.6093\n",
      "Fold 2 Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUPRC: 0.5815, AUC: 0.5000\n",
      "Validation Anomaly Scores (first 10): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training fold 3/5\n",
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 20, Loss: 0.6863\n",
      "Epoch 40, Loss: 0.6791\n",
      "Epoch 60, Loss: 0.6721\n",
      "Epoch 80, Loss: 0.6654\n",
      "Epoch 100, Loss: 0.6588\n",
      "Epoch 120, Loss: 0.6525\n",
      "Epoch 140, Loss: 0.6464\n",
      "Epoch 160, Loss: 0.6405\n",
      "Epoch 180, Loss: 0.6348\n",
      "Epoch 200, Loss: 0.6293\n",
      "Epoch 220, Loss: 0.6239\n",
      "Epoch 240, Loss: 0.6187\n",
      "Epoch 260, Loss: 0.6137\n",
      "Epoch 280, Loss: 0.6088\n",
      "Fold 3 Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUPRC: 0.5815, AUC: 0.5000\n",
      "Validation Anomaly Scores (first 10): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training fold 4/5\n",
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 20, Loss: 0.6863\n",
      "Epoch 40, Loss: 0.6791\n",
      "Epoch 60, Loss: 0.6720\n",
      "Epoch 80, Loss: 0.6653\n",
      "Epoch 100, Loss: 0.6587\n",
      "Epoch 120, Loss: 0.6524\n",
      "Epoch 140, Loss: 0.6463\n",
      "Epoch 160, Loss: 0.6404\n",
      "Epoch 180, Loss: 0.6346\n",
      "Epoch 200, Loss: 0.6291\n",
      "Epoch 220, Loss: 0.6237\n",
      "Epoch 240, Loss: 0.6185\n",
      "Epoch 260, Loss: 0.6134\n",
      "Epoch 280, Loss: 0.6085\n",
      "Fold 4 Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUPRC: 0.5815, AUC: 0.5000\n",
      "Validation Anomaly Scores (first 10): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Training fold 5/5\n",
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 20, Loss: 0.6863\n",
      "Epoch 40, Loss: 0.6790\n",
      "Epoch 60, Loss: 0.6720\n",
      "Epoch 80, Loss: 0.6652\n",
      "Epoch 100, Loss: 0.6587\n",
      "Epoch 120, Loss: 0.6523\n",
      "Epoch 140, Loss: 0.6462\n",
      "Epoch 160, Loss: 0.6403\n",
      "Epoch 180, Loss: 0.6345\n",
      "Epoch 200, Loss: 0.6290\n",
      "Epoch 220, Loss: 0.6236\n",
      "Epoch 240, Loss: 0.6184\n",
      "Epoch 260, Loss: 0.6133\n",
      "Epoch 280, Loss: 0.6084\n",
      "Fold 5 Precision: 0.0000, Recall: 0.0000, F1: 0.0000, AUPRC: 0.5815, AUC: 0.5000\n",
      "Validation Anomaly Scores (first 10): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Cross-validation results:\n",
      "Precision: 0.0000 (±0.0000)\n",
      "Recall: 0.0000 (±0.0000)\n",
      "F1: 0.0000 (±0.0000)\n",
      "Auprc: 0.5809 (±0.0012)\n",
      "Auc: 0.5002 (±0.0004)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Prepare data for GNN\n",
    "numeric_cols = [col for col in data_df.columns if col not in ['entity_id', 'is_anomaly', 'ml_flag_score', 'profile_type', 'agent_id']]\n",
    "X = data_df[numeric_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tensor = torch.FloatTensor(X_scaled)\n",
    "y_tensor = torch.FloatTensor(data_df['is_anomaly'].values).unsqueeze(1)\n",
    "\n",
    "# Create adjacency matrix with proper edge handling\n",
    "G = nx.DiGraph()\n",
    "for _, row in transactions.iterrows():\n",
    "    G.add_edge(row['sender_id'], row['receiver_id'], amount=row['amount'])\n",
    "if G.number_of_edges() != len(transactions):\n",
    "    print(f\"Warning: Graph edges ({G.number_of_edges()}) do not match transactions ({len(transactions)})\")\n",
    "adj = nx.adjacency_matrix(G, nodelist=data_df['entity_id'].tolist(), weight='amount')\n",
    "adj = sp.load_npz('adjacency_matrix.npz')\n",
    "adj_tensor = torch.sparse_csr_tensor(\n",
    "    torch.LongTensor(adj.indptr),\n",
    "    torch.LongTensor(adj.indices),\n",
    "    torch.FloatTensor(adj.data),\n",
    "    size=adj.shape\n",
    ").to_dense()\n",
    "\n",
    "# Define Enhanced GCN (GraphSAGE-inspired)\n",
    "class EnhancedGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features=1):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.conv1 = nn.Linear(in_features * 2, hidden_size1)  # Aggregate self + neighbors\n",
    "        self.conv2 = nn.Linear(hidden_size1 * 2, hidden_size2)\n",
    "        self.conv3 = nn.Linear(hidden_size2, out_features)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # First layer: aggregate neighbors\n",
    "        neighbor_agg = torch.mm(adj, x) / (torch.sum(adj, dim=1, keepdim=True) + 1e-7)\n",
    "        x = torch.cat([x, neighbor_agg], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        # Second layer\n",
    "        neighbor_agg = torch.mm(adj, x) / (torch.sum(adj, dim=1, keepdim=True) + 1e-7)\n",
    "        x = torch.cat([x, neighbor_agg], dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        # Output layer\n",
    "        x = self.conv3(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "model = EnhancedGCN(input_dim, 64, 16)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# K-fold cross-validation with stratification\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "metrics = {'precision': [], 'recall': [], 'f1': [], 'auprc': [], 'auc': []}\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, data_df['is_anomaly'])):\n",
    "    print(f\"Training fold {fold + 1}/5\")\n",
    "    X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]\n",
    "    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]\n",
    "    adj_train = adj_tensor[train_idx][:, train_idx]\n",
    "    adj_test = adj_tensor[test_idx][:, test_idx]\n",
    "\n",
    "    # Define institution mapping based on training data\n",
    "    train_df = data_df.iloc[train_idx].copy()\n",
    "    institution_cols = [col for col in train_df.columns if col.startswith('country_')]\n",
    "    institution_data = train_df[institution_cols].idxmax(axis=1).map(lambda x: x.replace('country_', '')).to_dict()\n",
    "    train_df['institution'] = train_df['entity_id'].map({k: v for k, v in institution_data.items() if v in ['US', 'EU', 'ASIA', 'HIGH_RISK']}).fillna('UNKNOWN')\n",
    "\n",
    "    # Local models per institution with weighted aggregation\n",
    "    local_models = {}\n",
    "    inst_weights = {'US': 0.3, 'EU': 0.3, 'ASIA': 0.2, 'HIGH_RISK': 0.2, 'UNKNOWN': 0.0}\n",
    "    for inst in train_df['institution'].unique():\n",
    "        inst_idx = train_df[train_df['institution'] == inst].index\n",
    "        if len(inst_idx) < 2:  # Skip small institutions\n",
    "            continue\n",
    "        relative_idx = np.searchsorted(train_idx, np.array(inst_idx))\n",
    "        relative_idx = relative_idx[relative_idx < len(train_idx)]\n",
    "        if len(relative_idx) < 2:\n",
    "            continue\n",
    "        local_X = X_train[relative_idx]\n",
    "        local_y = y_train[relative_idx]\n",
    "        local_adj = adj_train[relative_idx][:, relative_idx]\n",
    "        local_model = EnhancedGCN(input_dim, 64, 16)\n",
    "        local_opt = optim.Adam(local_model.parameters(), lr=0.001)\n",
    "        for epoch in range(200):  # Increased epochs\n",
    "            local_model.train()\n",
    "            outputs = local_model(local_X, local_adj)\n",
    "            loss = criterion(outputs, local_y)\n",
    "            local_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            local_opt.step()\n",
    "        local_models[inst] = (local_model, inst_weights.get(inst, 0.0))\n",
    "\n",
    "    # Aggregate weights with institution weighting\n",
    "    for param_name in model.state_dict().keys():\n",
    "        global_weight = torch.zeros_like(model.state_dict()[param_name])\n",
    "        total_weight = 0.0\n",
    "        for inst, (local_model, weight) in local_models.items():\n",
    "            global_weight += weight * local_model.state_dict()[param_name]\n",
    "            total_weight += weight\n",
    "        if total_weight > 0:\n",
    "            global_weight /= total_weight\n",
    "        model.state_dict()[param_name].copy_(global_weight)\n",
    "\n",
    "    # Train global model\n",
    "    model.train()\n",
    "    for epoch in range(300):  # Increased epochs\n",
    "        outputs = model(X_train, adj_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test, adj_test)\n",
    "        preds = test_outputs.numpy().flatten()\n",
    "        y_true = y_test.numpy().flatten()\n",
    "\n",
    "    # Dynamic threshold using Youden’s J\n",
    "    if y_true.sum() > 1:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, preds)\n",
    "        optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    else:\n",
    "        print(f\"Warning: Too few positive samples in fold {fold + 1}; using 95th percentile\")\n",
    "        optimal_threshold = np.percentile(preds, 95)\n",
    "    pred_labels = (preds > optimal_threshold).astype(int)\n",
    "\n",
    "    # Normalize anomaly scores\n",
    "    preds = (preds - preds.min()) / (preds.max() - preds.min() + 1e-7)\n",
    "\n",
    "    # Metrics\n",
    "    precision = precision_score(y_true, pred_labels, zero_division=0)\n",
    "    recall = recall_score(y_true, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(y_true, pred_labels, zero_division=0)\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, preds)\n",
    "    auprc = auc(recall_vals, precision_vals)\n",
    "    fpr, tpr, _ = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['f1'].append(f1)\n",
    "    metrics['auprc'].append(auprc)\n",
    "    metrics['auc'].append(roc_auc)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Validation Anomaly Scores (first 10): {preds[:10]}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nCross-validation results:\")\n",
    "for metric, values in metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {np.mean(values):.4f} (±{np.std(values):.4f})\")\n",
    "\n",
    "# Save predictions\n",
    "data_df['anomaly_score'] = np.zeros(len(data_df))\n",
    "data_df.loc[test_idx, 'anomaly_score'] = preds\n",
    "data_df['predicted_anomaly'] = (data_df['anomaly_score'] > optimal_threshold).astype(int)\n",
    "data_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
