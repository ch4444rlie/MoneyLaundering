{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import kuzu\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import shutil\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_entities=5000, num_transactions=25000):\n",
    "    np.random.seed(42)\n",
    "    # Step 1: Benign data\n",
    "    countries = np.random.choice(['US', 'EU', 'ASIA', 'HIGH_RISK'], num_entities, p=[0.4, 0.3, 0.2, 0.1])\n",
    "    entities = pd.DataFrame({\n",
    "        'entity_id': [f'E{i:04d}' for i in range(num_entities)],\n",
    "        'profile_type': np.random.choice(['individual_low', 'individual_high', 'business_small', 'business_large'], num_entities, p=[0.4, 0.1, 0.3, 0.2]),\n",
    "        'country': countries,\n",
    "        'agent_id': np.random.randint(1, 100, num_entities),\n",
    "        'kyc_risk_score': np.random.uniform(0, 0.3, num_entities),\n",
    "        'dormancy_period': np.random.randint(0, 365, num_entities)\n",
    "    })\n",
    "    G = nx.barabasi_albert_graph(num_entities, 5, seed=42)  # Benign network\n",
    "    edges = [(f'E{u:04d}', f'E{v:04d}') for u, v in G.edges()]\n",
    "    transactions = pd.DataFrame({\n",
    "        'sender_id': [edges[i % len(edges)][0] for i in range(num_transactions)],\n",
    "        'receiver_id': [edges[i % len(edges)][1] for i in range(num_transactions)],\n",
    "        'amount': np.random.exponential(50000, num_transactions).clip(100, 1500000),\n",
    "        'timestamp': np.random.randint(0, 30, num_transactions),\n",
    "        'ml_flag': np.zeros(num_transactions, dtype=int),\n",
    "        'typology': ['benign'] * num_transactions  # Track typology\n",
    "    })\n",
    "    # Add cross-border flag based on countries\n",
    "    transactions['is_cross_border'] = transactions.apply(lambda row: entities.loc[entities['entity_id'] == row['sender_id'], 'country'].values[0] != entities.loc[entities['entity_id'] == row['receiver_id'], 'country'].values[0], axis=1)\n",
    "    transactions['high_risk_jurisdiction'] = transactions.apply(lambda row: 'HIGH_RISK' in [entities.loc[entities['entity_id'] == row['sender_id'], 'country'].values[0], entities.loc[entities['entity_id'] == row['receiver_id'], 'country'].values[0]], axis=1)\n",
    "    transactions['flagged_receiver'] = np.random.choice([True, False], num_transactions, p=[0.02, 0.98])\n",
    "\n",
    "    # Step 2: Inject typologies (1% suspicious entities, as per low ML estimates)\n",
    "    num_suspicious = int(0.01 * num_entities)\n",
    "    suspicious_ids = np.random.choice(entities['entity_id'], num_suspicious, replace=False)\n",
    "    for sid in suspicious_ids:\n",
    "        # Common suspicious traits\n",
    "        entities.loc[entities['entity_id'] == sid, 'kyc_risk_score'] = np.random.uniform(0.6, 0.9)\n",
    "        entities.loc[entities['entity_id'] == sid, 'dormancy_period'] = np.random.randint(200, 365)\n",
    "        ts = np.random.randint(0, 3)\n",
    "        \n",
    "        # Typology 1: Smurfing (split large amount into small tx to many receivers)\n",
    "        large_amount = np.random.uniform(200000, 800000)\n",
    "        num_smurfs = np.random.randint(10, 20)\n",
    "        small_amounts = np.full(num_smurfs, large_amount / num_smurfs)\n",
    "        receivers = np.random.choice(entities['entity_id'], num_smurfs, replace=False)\n",
    "        smurf_tx = pd.DataFrame({\n",
    "            'sender_id': [sid] * num_smurfs,\n",
    "            'receiver_id': receivers,\n",
    "            'amount': small_amounts,\n",
    "            'timestamp': [ts] * num_smurfs,\n",
    "            'ml_flag': 1,\n",
    "            'is_cross_border': np.random.choice([True, False], num_smurfs, p=[0.7, 0.3]),\n",
    "            'high_risk_jurisdiction': np.random.choice([True, False], num_smurfs, p=[0.5, 0.5]),\n",
    "            'flagged_receiver': np.random.choice([True, False], num_smurfs, p=[0.3, 0.7]),\n",
    "            'typology': ['smurfing'] * num_smurfs\n",
    "        })\n",
    "        transactions = pd.concat([transactions, smurf_tx], ignore_index=True)\n",
    "        \n",
    "        # Typology 2: Money Mules (recruit mules to forward funds)\n",
    "        num_mules = np.random.randint(3, 6)\n",
    "        mules = np.random.choice(entities['entity_id'], num_mules, replace=False)\n",
    "        mule_amount = np.random.uniform(5000, 50000)\n",
    "        for mule in mules:\n",
    "            # Suspicious -> Mule -> Final (back or to high-risk)\n",
    "            final = np.random.choice(entities[entities['country'] == 'HIGH_RISK']['entity_id'], 1)[0] if np.random.rand() > 0.5 else sid  # Round-trip\n",
    "            mule_tx = pd.DataFrame({\n",
    "                'sender_id': [sid, mule],\n",
    "                'receiver_id': [mule, final],\n",
    "                'amount': [mule_amount] * 2,\n",
    "                'timestamp': [ts, ts + 1],\n",
    "                'ml_flag': 1,\n",
    "                'is_cross_border': [False, True],\n",
    "                'high_risk_jurisdiction': [False, True],\n",
    "                'flagged_receiver': [True, False],\n",
    "                'typology': ['money_mule'] * 2\n",
    "            })\n",
    "            transactions = pd.concat([transactions, mule_tx], ignore_index=True)\n",
    "        \n",
    "        # Typology 3: CLS (complex layering: multi-hop chains/loops)\n",
    "        layers = np.random.randint(4, 7)\n",
    "        chain = [sid] + list(np.random.choice(entities['entity_id'], layers - 1, replace=False)) + [sid if np.random.rand() > 0.5 else np.random.choice(entities['entity_id'])]\n",
    "        layer_amount = np.random.uniform(10000, 100000)\n",
    "        for j in range(len(chain) - 1):\n",
    "            cls_tx = pd.DataFrame({\n",
    "                'sender_id': [chain[j]],\n",
    "                'receiver_id': [chain[j+1]],\n",
    "                'amount': [layer_amount],\n",
    "                'timestamp': [ts + j],\n",
    "                'ml_flag': 1,\n",
    "                'is_cross_border': np.random.choice([True, False], 1, p=[0.8, 0.2]),\n",
    "                'high_risk_jurisdiction': np.random.choice([True, False], 1, p=[0.6, 0.4]),\n",
    "                'flagged_receiver': np.random.choice([True, False], 1, p=[0.4, 0.6]),\n",
    "                'typology': ['cls']\n",
    "            })\n",
    "            transactions = pd.concat([transactions, cls_tx], ignore_index=True)\n",
    "\n",
    "    # Step 3: Refine realism (add noise to 10% benign, adjust for Phase 2's real-world focus)\n",
    "    noise_idx = np.random.choice(transactions[transactions['typology'] == 'benign'].index, int(0.1 * num_transactions))\n",
    "    transactions.loc[noise_idx, 'amount'] *= np.random.uniform(0.8, 1.2)\n",
    "    transactions.loc[noise_idx, 'is_cross_border'] = np.random.choice([True, False], len(noise_idx), p=[0.4, 0.6])\n",
    "    # Additional noise for high-risk variability (inspired by 2025 updates on cross-border integrity)\n",
    "    high_risk_noise = transactions[transactions['high_risk_jurisdiction']]\n",
    "    if not high_risk_noise.empty:\n",
    "        high_risk_noise_idx = np.random.choice(high_risk_noise.index, int(0.2 * len(high_risk_noise)))\n",
    "        transactions.loc[high_risk_noise_idx, 'flagged_receiver'] = True\n",
    "\n",
    "    entities.to_csv('entities.csv', index=False)\n",
    "    transactions.to_csv('transactions.csv', index=False)\n",
    "    print(f\"Generated {num_entities} entities and {len(transactions)} transactions (after typologies)\")\n",
    "    print(transactions['typology'].value_counts())\n",
    "    print(transactions['amount'].describe())\n",
    "    print(transactions[['is_cross_border', 'ml_flag', 'flagged_receiver', 'high_risk_jurisdiction']].value_counts())\n",
    "    print(entities[['kyc_risk_score', 'dormancy_period']].describe())\n",
    "    print(f\"Proportion of ml_flag = 1: {transactions['ml_flag'].mean():.4f}\")\n",
    "    return entities, transactions\n",
    "\n",
    "# Call the function to generate data\n",
    "entities, transactions = generate_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3a\n",
    "\n",
    "\n",
    "# Load data (if not already in memory from Cell 2)\n",
    "entities = pd.read_csv('entities.csv')\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "\n",
    "# Build directed graph (since transactions have direction)\n",
    "G = nx.DiGraph()\n",
    "# Add nodes with attributes\n",
    "for _, row in entities.iterrows():\n",
    "    G.add_node(row['entity_id'], **row.to_dict())\n",
    "# Add edges with attributes\n",
    "for _, row in transactions.iterrows():\n",
    "    G.add_edge(row['sender_id'], row['receiver_id'], **row.to_dict())\n",
    "\n",
    "# Verify graph\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "print(f\"Graph built with {num_nodes} nodes and {num_edges} edges\")\n",
    "if num_nodes == 0 or num_edges == 0:\n",
    "    raise ValueError(\"Graph is empty.\")\n",
    "\n",
    "# Feature extraction (replacing Kuzu queries)\n",
    "\n",
    "# Degree (out-degree, since directed)\n",
    "degree_dict = dict(G.out_degree())\n",
    "degree_df = pd.DataFrame({'entity_id': list(degree_dict.keys()), 'degree': list(degree_dict.values())}).fillna(0)\n",
    "\n",
    "# In-degree (for later features)\n",
    "in_degree_dict = dict(G.in_degree())\n",
    "in_degree_df = pd.DataFrame({'entity_id': list(in_degree_dict.keys()), 'in_degree': list(in_degree_dict.values())}).fillna(0)\n",
    "\n",
    "# Small transaction count (<1000)\n",
    "small_tx = {}\n",
    "for node in G.nodes():\n",
    "    small_tx[node] = sum(1 for _, _, data in G.out_edges(node, data=True) if data.get('amount', 0) < 1000)\n",
    "small_tx_df = pd.DataFrame({'entity_id': list(small_tx.keys()), 'small_tx_count': list(small_tx.values())}).fillna(0)\n",
    "\n",
    "# Clustering coefficient (using undirected version for simplicity, as in your query)\n",
    "undirected_G = G.to_undirected()\n",
    "clustering_dict = nx.clustering(undirected_G)\n",
    "cluster_df = pd.DataFrame({'entity_id': list(clustering_dict.keys()), 'clustering_coeff': list(clustering_dict.values())}).fillna(0)\n",
    "\n",
    "# Transaction frequency variance (based on out-edges count)\n",
    "tx_count = {node: G.out_degree(node) for node in G.nodes()}\n",
    "tx_count_df = pd.DataFrame({'entity_id': list(tx_count.keys()), 'tx_count': list(tx_count.values())}).fillna(0)\n",
    "mean_tx_count = tx_count_df['tx_count'].mean()\n",
    "tx_count_df['tx_freq_variance'] = np.sqrt((tx_count_df['tx_count'] - mean_tx_count) ** 2).fillna(0)\n",
    "tx_freq_df = tx_count_df[['entity_id', 'tx_freq_variance']]\n",
    "\n",
    "# Amount variance, skewness, avg, high_value_ratio\n",
    "amount_data = {}\n",
    "for node in G.nodes():\n",
    "    amounts = [data.get('amount', 0) for _, _, data in G.out_edges(node, data=True)]\n",
    "    if len(amounts) > 0:\n",
    "        avg = np.mean(amounts)\n",
    "        var = np.var(amounts, ddof=1) if len(amounts) > 1 else 0\n",
    "        skew = (((np.array(amounts) - avg) ** 3).mean()) / (np.std(amounts, ddof=1) ** 3 + 1e-7) if len(amounts) > 1 else 0\n",
    "        high_ratio = sum(a > 100000 for a in amounts) / len(amounts)\n",
    "    else:\n",
    "        avg = var = skew = high_ratio = 0\n",
    "    amount_data[node] = {'avg_amount': avg, 'amount_variance': var, 'amount_skewness': skew, 'high_value_ratio': high_ratio}\n",
    "var_skew_df = pd.DataFrame.from_dict(amount_data, orient='index').reset_index().rename(columns={'index': 'entity_id'}).fillna(0)\n",
    "\n",
    "# Temporal features: velocity, burstiness, concentration\n",
    "time_data = {}\n",
    "for node in G.nodes():\n",
    "    timestamps = [data.get('timestamp', 0) for _, _, data in G.out_edges(node, data=True)]\n",
    "    if len(timestamps) > 0:\n",
    "        min_t, max_t = min(timestamps), max(timestamps)\n",
    "        velocity = len(timestamps) / (max_t - min_t + 1e-7)\n",
    "        burst = np.var(timestamps, ddof=1) * 10 if len(timestamps) > 1 else 0\n",
    "        conc = 1 if (max_t - min_t < 3) else 0  # Binary as per your mean logic\n",
    "    else:\n",
    "        velocity = burst = conc = 0\n",
    "    time_data[node] = {'tx_velocity': velocity, 'burstiness': burst, 'temporal_concentration': conc}\n",
    "time_agg_df = pd.DataFrame.from_dict(time_data, orient='index').reset_index().rename(columns={'index': 'entity_id'}).fillna(0)\n",
    "\n",
    "# Additional features (kyc, dormancy, cross_border_amount, tx_frequency, directionality_ratio)\n",
    "features = {}\n",
    "for node in G.nodes():\n",
    "    node_data = G.nodes[node]\n",
    "    out_edges = list(G.out_edges(node, data=True))\n",
    "    in_edges = list(G.in_edges(node, data=True))\n",
    "    cross_border_amount = sum(data.get('amount', 0) for _, _, data in out_edges if data.get('is_cross_border', False))\n",
    "    tx_frequency = len(out_edges)\n",
    "    directionality_ratio = len(in_edges) / (len(in_edges) + tx_frequency + 1e-7)\n",
    "    features[node] = {\n",
    "        'kyc_risk_score': node_data.get('kyc_risk_score', 0),\n",
    "        'dormancy_period': node_data.get('dormancy_period', 0),\n",
    "        'cross_border_amount': cross_border_amount,\n",
    "        'in_degree': len(in_edges),\n",
    "        'tx_frequency': tx_frequency,\n",
    "        'directionality_ratio': directionality_ratio\n",
    "    }\n",
    "features_df = pd.DataFrame.from_dict(features, orient='index').reset_index().rename(columns={'index': 'entity_id'}).fillna(0)\n",
    "\n",
    "# Round-tripping (count 2-hop cycles back to self)\n",
    "round_trip = {}\n",
    "for node in G.nodes():\n",
    "    count = 0\n",
    "    for neighbor in G.neighbors(node):\n",
    "        for next_neighbor in G.neighbors(neighbor):\n",
    "            if next_neighbor == node and neighbor != node:\n",
    "                count += 1\n",
    "    round_trip[node] = count\n",
    "round_trip_df = pd.DataFrame({'entity_id': list(round_trip.keys()), 'round_trip_count': list(round_trip.values())}).fillna(0)\n",
    "\n",
    "# Ensure all DFs cover all entities (add missing with defaults)\n",
    "all_entities = pd.DataFrame({'entity_id': entities['entity_id']})\n",
    "degree_df = all_entities.merge(degree_df, on='entity_id', how='left').fillna(0)\n",
    "# Repeat for others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3b\n",
    "\n",
    "# Merge features from Cell 3\n",
    "data_df = degree_df.merge(small_tx_df, on='entity_id', how='left')\\\n",
    "                   .merge(var_skew_df, on='entity_id', how='left')\\\n",
    "                   .merge(cluster_df, on='entity_id', how='left')\\\n",
    "                   .merge(tx_freq_df, on='entity_id', how='left')\\\n",
    "                   .merge(time_agg_df, on='entity_id', how='left')\\\n",
    "                   .merge(features_df, on='entity_id', how='left')\\\n",
    "                   .merge(round_trip_df, on='entity_id', how='left')\\\n",
    "                   .merge(in_degree_df, on='entity_id', how='left')\n",
    "\n",
    "# Fill missing values (e.g., entities with no transactions)\n",
    "data_df.fillna({\n",
    "    'degree': 0, 'small_tx_count': 0, 'avg_amount': 0, 'amount_variance': 0, \n",
    "    'amount_skewness': 0, 'high_value_ratio': 0, 'clustering_coeff': 0, \n",
    "    'tx_freq_variance': 0, 'tx_velocity': 0, 'burstiness': 0, \n",
    "    'temporal_concentration': 0, 'kyc_risk_score': 0, 'dormancy_period': 0, \n",
    "    'cross_border_amount': 0, 'in_degree': 0, 'tx_frequency': 0, \n",
    "    'directionality_ratio': 0, 'round_trip_count': 0\n",
    "}, inplace=True)\n",
    "\n",
    "# Compute ml_flag_score for anomaly labeling\n",
    "ml_flag_dict = {}\n",
    "for node in G.nodes():\n",
    "    ml_flags = [data.get('ml_flag', 0) for _, _, data in G.out_edges(node, data=True)]\n",
    "    ml_flag_dict[node] = np.mean(ml_flags) if ml_flags else 0\n",
    "ml_flag_df = pd.DataFrame({'entity_id': list(ml_flag_dict.keys()), 'ml_flag_score': list(ml_flag_dict.values())}).fillna(0)\n",
    "\n",
    "data_df = data_df.merge(ml_flag_df, on='entity_id', how='left').fillna({'ml_flag_score': 0})\n",
    "\n",
    "# Define anomaly labels using ROC-optimized threshold\n",
    "data_df['is_anomaly'] = (data_df['ml_flag_score'] > 0).astype(int)\n",
    "if data_df['is_anomaly'].sum() > 1:  # Ensure enough positives\n",
    "    fpr, tpr, thresholds = roc_curve(data_df['is_anomaly'], data_df['ml_flag_score'])\n",
    "    anomaly_threshold = thresholds[np.argmax(tpr - fpr)]  # Youdenâ€™s J\n",
    "    data_df['is_anomaly'] = (data_df['ml_flag_score'] > anomaly_threshold).astype(int)\n",
    "else:\n",
    "    print(\"Warning: Too few positive samples; using 95th percentile threshold\")\n",
    "    anomaly_threshold = np.percentile(data_df['ml_flag_score'], 95)\n",
    "    data_df['is_anomaly'] = (data_df['ml_flag_score'] > anomaly_threshold).astype(int)\n",
    "\n",
    "# Validate features\n",
    "print(\"Merged data_df shape:\", data_df.shape)\n",
    "print(\"NaN counts:\\n\", data_df.isna().sum())\n",
    "print(\"Data types:\\n\", data_df.dtypes)\n",
    "print(\"Feature stats:\\n\", data_df.describe())\n",
    "if not np.all(np.isfinite(data_df.drop(columns=['entity_id']).select_dtypes(include=[np.number]))):\n",
    "    print(\"Warning: Non-finite values detected; replacing with 0\")\n",
    "    data_df.update(data_df.select_dtypes(include=[np.number]).fillna(0))\n",
    "\n",
    "# Check for leakage (high correlation with label)\n",
    "correlations = data_df.drop(columns=['entity_id', 'is_anomaly', 'ml_flag_score']).corrwith(data_df['ml_flag_score'])\n",
    "print(\"Feature correlations with ml_flag_score:\\n\", correlations)\n",
    "\n",
    "# Save for debugging\n",
    "data_df.to_csv('merged_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Simulation across institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate institutions (e.g., 3 banks)\n",
    "num_insts = 3\n",
    "inst_labels = np.random.randint(0, num_insts, len(data_df))\n",
    "global_model = Autoencoder(input_dim)  # Or your GCN\n",
    "\n",
    "for inst in range(num_insts):\n",
    "    inst_idx = np.where(inst_labels == inst)[0]\n",
    "    inst_train_idx = np.intersect1d(train_idx, inst_idx)\n",
    "    if len(inst_train_idx) == 0: continue\n",
    "    local_X = X[inst_train_idx]\n",
    "    local_normal = local_X[data_df.iloc[inst_train_idx]['is_anomaly'] == 0]\n",
    "    local_tensor = torch.FloatTensor(local_normal)\n",
    "    \n",
    "    local_model = Autoencoder(input_dim)  # Train local\n",
    "    local_opt = optim.Adam(local_model.parameters(), lr=0.0005)\n",
    "    for epoch in range(100):  # Shorter local epochs\n",
    "        outputs = local_model(local_tensor)\n",
    "        loss = criterion(outputs, local_tensor)\n",
    "        local_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        local_opt.step()\n",
    "    \n",
    "    # Average to global (simple FL)\n",
    "    for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n",
    "        global_param.data = (global_param.data + local_param.data) / 2  # Or weighted\n",
    "\n",
    "# Use global_model for inference as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
