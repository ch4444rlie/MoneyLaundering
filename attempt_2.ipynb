{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kuzu\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Autoencoder\n",
    "\n",
    "# Autoencoder model with increased dropout and deeper architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 entities and 25000 transactions\n",
      "count     25004.000000\n",
      "mean      56483.247940\n",
      "std       76459.887197\n",
      "min         100.000000\n",
      "25%       15833.933555\n",
      "50%       36874.917196\n",
      "75%       71979.541353\n",
      "max      795934.101611\n",
      "Name: amount, dtype: float64\n",
      "is_cross_border  ml_flag  flagged_receiver  high_risk_jurisdiction\n",
      "False            0        False             False                     20668\n",
      "True             0        False             False                      2989\n",
      "False            0        False             True                        650\n",
      "                          True              False                       439\n",
      "True             0        False             True                        120\n",
      "                          True              False                        65\n",
      "False            0        True              True                         36\n",
      "True             0        True              True                         15\n",
      "                 1        False             False                         8\n",
      "                          True              False                         5\n",
      "False            1        False             False                         2\n",
      "                          True              False                         2\n",
      "                          False             True                          2\n",
      "True             1        False             True                          2\n",
      "                          True              True                          1\n",
      "Name: count, dtype: int64\n",
      "       kyc_risk_score  dormancy_period\n",
      "count     5000.000000      5000.000000\n",
      "mean         0.161833       182.736200\n",
      "std          0.109571       105.010205\n",
      "min          0.000036         0.000000\n",
      "25%          0.079090        90.000000\n",
      "50%          0.155617       184.000000\n",
      "75%          0.231776       273.000000\n",
      "max          0.894996       364.000000\n",
      "Proportion of ml_flag = 1: 0.0009\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def generate_synthetic_data(num_entities=5000, num_transactions=25000):\n",
    "    np.random.seed(42)\n",
    "    entities = pd.DataFrame({\n",
    "        'entity_id': [f'E{i:04d}' for i in range(num_entities)],\n",
    "        'profile_type': np.random.choice(['individual_low', 'individual_high', 'business_small', 'business_large'], num_entities, p=[0.4, 0.1, 0.3, 0.2]),\n",
    "        'agent_id': np.random.randint(1, 100, num_entities),\n",
    "        'kyc_risk_score': np.random.uniform(0, 0.3, num_entities),\n",
    "        'dormancy_period': np.random.randint(0, 365, num_entities)\n",
    "    })\n",
    "    suspicious = np.random.choice([0, 1], num_entities, p=[0.99, 0.01])\n",
    "    potential_launderers = np.random.choice([0, 1], num_entities, p=[0.98, 0.02])\n",
    "    suspicious_indices = np.where(suspicious)[0]\n",
    "    potential_indices = np.where(potential_launderers & ~suspicious)[0]\n",
    "    \n",
    "    entities.loc[suspicious_indices, 'kyc_risk_score'] = np.where(\n",
    "        entities.loc[suspicious_indices, 'profile_type'].isin(['individual_low', 'individual_high']),\n",
    "        np.random.uniform(0.6, 0.9, len(suspicious_indices)),\n",
    "        np.random.uniform(0.4, 0.7, len(suspicious_indices))\n",
    "    )\n",
    "    entities.loc[suspicious_indices, 'dormancy_period'] = np.random.randint(200, 365, len(suspicious_indices))\n",
    "    entities.loc[potential_indices, 'kyc_risk_score'] = np.random.uniform(0.3, 0.6, len(potential_indices))\n",
    "    \n",
    "    G = nx.barabasi_albert_graph(num_entities, 5, seed=42)\n",
    "    edges = [(f'E{u:04d}', f'E{v:04d}') for u, v in G.edges()]\n",
    "    transactions = pd.DataFrame({\n",
    "        'sender_id': [edges[i % len(edges)][0] for i in range(num_transactions)],\n",
    "        'receiver_id': [edges[i % len(edges)][1] for i in range(num_transactions)],\n",
    "        'amount': np.random.exponential(50000, num_transactions).clip(100, 1500000),\n",
    "        'is_cross_border': np.random.choice([True, False], num_transactions, p=[0.08, 0.92]),\n",
    "        'timestamp': np.random.randint(0, 30, num_transactions),\n",
    "        'ml_flag': np.zeros(num_transactions, dtype=int),\n",
    "        'flagged_receiver': np.random.choice([True, False], num_transactions, p=[0.02, 0.98]),\n",
    "        'high_risk_jurisdiction': np.random.choice([True, False], num_transactions, p=[0.03, 0.97])\n",
    "    })\n",
    "    \n",
    "    for i, row in entities.iterrows():\n",
    "        mask = transactions['sender_id'] == row['entity_id']\n",
    "        num_tx = sum(mask)\n",
    "        if suspicious[i]:\n",
    "            transactions.loc[mask, 'ml_flag'] = np.random.choice([0, 1], num_tx, p=[0.9, 0.1])\n",
    "            transactions.loc[mask, 'amount'] = np.random.choice(\n",
    "                [np.random.uniform(5000, 50000), np.random.uniform(200000, 800000)], \n",
    "                size=num_tx, p=[0.6, 0.4]\n",
    "            )\n",
    "            transactions.loc[mask, 'timestamp'] = np.random.randint(0, 3, num_tx)\n",
    "            transactions.loc[mask, 'is_cross_border'] = np.random.choice([True, False], num_tx, p=[0.7, 0.3])\n",
    "            transactions.loc[mask, 'high_risk_jurisdiction'] = np.random.choice([True, False], num_tx, p=[0.5, 0.5])\n",
    "            transactions.loc[mask, 'flagged_receiver'] = np.random.choice([True, False], num_tx, p=[0.3, 0.7])\n",
    "        elif potential_launderers[i]:\n",
    "            transactions.loc[mask, 'amount'] = np.random.choice(\n",
    "                [np.random.uniform(5000, 50000), np.random.uniform(200000, 800000)], \n",
    "                size=num_tx, p=[0.6, 0.4]\n",
    "            )\n",
    "            transactions.loc[mask, 'timestamp'] = np.random.randint(0, 3, num_tx)\n",
    "            transactions.loc[mask, 'is_cross_border'] = np.random.choice([True, False], num_tx, p=[0.7, 0.3])\n",
    "    \n",
    "    suspicious_transactions = transactions[transactions['ml_flag'] == 1].sample(frac=0.1)\n",
    "    for idx in suspicious_transactions.index:\n",
    "        sender = transactions.loc[idx, 'sender_id']\n",
    "        intermediate = np.random.choice(entities['entity_id'], 1)[0]\n",
    "        transactions = pd.concat([transactions, pd.DataFrame([{\n",
    "            'sender_id': sender, 'receiver_id': intermediate, 'amount': transactions.loc[idx, 'amount'],\n",
    "            'is_cross_border': False, 'timestamp': transactions.loc[idx, 'timestamp'] + 1,\n",
    "            'ml_flag': 0, 'flagged_receiver': False, 'high_risk_jurisdiction': False\n",
    "        }, {\n",
    "            'sender_id': intermediate, 'receiver_id': sender, 'amount': transactions.loc[idx, 'amount'],\n",
    "            'is_cross_border': False, 'timestamp': transactions.loc[idx, 'timestamp'] + 2,\n",
    "            'ml_flag': 0, 'flagged_receiver': False, 'high_risk_jurisdiction': False\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    normal_high_risk = np.random.choice(entities.index, size=int(0.1 * num_entities))\n",
    "    for i in normal_high_risk:\n",
    "        mask = transactions['sender_id'] == entities.loc[i, 'entity_id']\n",
    "        transactions.loc[mask, 'is_cross_border'] = np.random.choice([True, False], sum(mask), p=[0.4, 0.6])\n",
    "        transactions.loc[mask, 'amount'] = np.random.uniform(10000, 100000, sum(mask))\n",
    "    \n",
    "    entities.to_csv('entities.csv', index=False)\n",
    "    transactions.to_csv('transactions.csv', index=False)\n",
    "    print(f\"Generated {num_entities} entities and {num_transactions} transactions\")\n",
    "    print(transactions['amount'].describe())\n",
    "    print(transactions[['is_cross_border', 'ml_flag', 'flagged_receiver', 'high_risk_jurisdiction']].value_counts())\n",
    "    print(entities[['kyc_risk_score', 'dormancy_period']].describe())\n",
    "    print(f\"Proportion of ml_flag = 1: {transactions['ml_flag'].mean():.4f}\")\n",
    "\n",
    "generate_synthetic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Kuzu database\n",
      "Number of entities: 5000\n",
      "Number of transactions: 25004\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Clear and recreate Kuzu database\n",
    "db_path = 'kuzu_db'\n",
    "if os.path.exists(db_path):\n",
    "    shutil.rmtree(db_path)\n",
    "print(\"Creating new Kuzu database\")\n",
    "db = kuzu.Database(db_path)\n",
    "conn = kuzu.Connection(db)\n",
    "conn.execute(\"CREATE NODE TABLE Entity(entity_id STRING, profile_type STRING, agent_id INT, kyc_risk_score DOUBLE, dormancy_period INT, PRIMARY KEY(entity_id))\")\n",
    "conn.execute(\"CREATE REL TABLE Transaction(FROM Entity TO Entity, amount DOUBLE, is_cross_border BOOLEAN, timestamp INT, ml_flag INT, flagged_receiver BOOLEAN, high_risk_jurisdiction BOOLEAN)\")\n",
    "conn.execute(\"COPY Entity FROM 'entities.csv' (HEADER=TRUE, DELIM=',', QUOTE='\\\"')\")\n",
    "conn.execute(\"COPY Transaction FROM 'transactions.csv' (HEADER=TRUE, DELIM=',', QUOTE='\\\"')\")\n",
    "\n",
    "# Verify database\n",
    "result = conn.execute(\"MATCH (e:Entity) RETURN COUNT(e)\")\n",
    "num_entities = result.get_next()[0]\n",
    "result = conn.execute(\"MATCH ()-[t:Transaction]->() RETURN COUNT(t)\")\n",
    "num_transactions = result.get_next()[0]\n",
    "print(f\"Number of entities: {num_entities}\")\n",
    "print(f\"Number of transactions: {num_transactions}\")\n",
    "if num_entities == 0 or num_transactions == 0:\n",
    "    raise ValueError(\"Database is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree query returned 3534 rows\n",
      "Small transaction query returned 337 rows\n",
      "Clustering coefficient query returned 3 rows\n",
      "Transaction frequency query returned 3534 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "\n",
    "# Query 1: Degree\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t:Transaction]->()\n",
    "        RETURN e.entity_id, COUNT(t) AS degree\n",
    "    \"\"\")\n",
    "    degree_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                            columns=['entity_id', 'degree'])\n",
    "    degree_df['degree'] = pd.to_numeric(degree_df['degree'], errors='coerce').fillna(0)\n",
    "    print(f\"Degree query returned {len(degree_df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing degree query: {e}\")\n",
    "    degree_df = pd.DataFrame({'entity_id': entities['entity_id'], 'degree': 0})\n",
    "\n",
    "# Query 3: Small transaction count\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t:Transaction]->()\n",
    "        WHERE t.amount < 1000\n",
    "        RETURN e.entity_id, COUNT(t) AS small_tx_count\n",
    "    \"\"\")\n",
    "    small_tx_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                              columns=['entity_id', 'small_tx_count'])\n",
    "    small_tx_df['small_tx_count'] = pd.to_numeric(small_tx_df['small_tx_count'], errors='coerce').fillna(0)\n",
    "    print(f\"Small transaction query returned {len(small_tx_df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing small transaction query: {e}\")\n",
    "    small_tx_df = pd.DataFrame({'entity_id': entities['entity_id'], 'small_tx_count': 0})\n",
    "\n",
    "# Query 4: Clustering coefficient\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t1:Transaction]->(m:Entity)-[t2:Transaction]->(n:Entity)\n",
    "        WHERE n.entity_id = e.entity_id AND m.entity_id <> e.entity_id\n",
    "        RETURN e.entity_id, COUNT(DISTINCT m) AS clustering_coeff\n",
    "    \"\"\")\n",
    "    cluster_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                             columns=['entity_id', 'clustering_coeff'])\n",
    "    cluster_df['clustering_coeff'] = pd.to_numeric(cluster_df['clustering_coeff'], errors='coerce').fillna(0)\n",
    "    print(f\"Clustering coefficient query returned {len(cluster_df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing clustering coefficient query: {e}\")\n",
    "    cluster_df = pd.DataFrame({'entity_id': entities['entity_id'], 'clustering_coeff': 0})\n",
    "\n",
    "# Query 5: Transaction frequency variance\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t:Transaction]->()\n",
    "        RETURN e.entity_id, COUNT(t) AS tx_count\n",
    "    \"\"\")\n",
    "    tx_count_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                              columns=['entity_id', 'tx_count'])\n",
    "    tx_count_df['tx_count'] = pd.to_numeric(tx_count_df['tx_count'], errors='coerce').fillna(0)\n",
    "    mean_tx_count = tx_count_df['tx_count'].mean()\n",
    "    tx_count_df['tx_freq_variance'] = (tx_count_df['tx_count'] - mean_tx_count) ** 2\n",
    "    tx_freq_df = tx_count_df.groupby('entity_id').agg({'tx_freq_variance': 'mean'}).reset_index()\n",
    "    tx_freq_df['tx_freq_variance'] = np.sqrt(tx_freq_df['tx_freq_variance']).fillna(0)\n",
    "    print(f\"Transaction frequency query returned {len(tx_count_df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error computing tx frequency variance: {e}\")\n",
    "    tx_freq_df = pd.DataFrame({'entity_id': entities['entity_id'], 'tx_freq_variance': 0})\n",
    "\n",
    "# Query 6: Amount variance and skewness\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t:Transaction]->()\n",
    "        RETURN e.entity_id, t.amount AS amount\n",
    "    \"\"\")\n",
    "    amount_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                            columns=['entity_id', 'amount'])\n",
    "    amount_df['amount'] = pd.to_numeric(amount_df['amount'], errors='coerce')\n",
    "    var_skew_df = amount_df.groupby('entity_id').agg({\n",
    "        'amount': [\n",
    "            ('avg_amount', 'mean'),\n",
    "            ('amount_variance', lambda x: np.var(x, ddof=1) if len(x) > 1 else 0),\n",
    "            ('amount_skewness', lambda x: ((x - x.mean()) ** 3).mean() / (x.std(ddof=1) ** 3 + 1e-7) if len(x) > 1 else 0),\n",
    "            ('high_value_ratio', lambda x: sum(x > 100000) / len(x) if len(x) > 0 else 0)  # Added\n",
    "        ]\n",
    "    }).droplevel(0, axis=1).reset_index()\n",
    "    var_skew_df[['amount_variance', 'amount_skewness', 'high_value_ratio']] = var_skew_df[['amount_variance', 'amount_skewness', 'high_value_ratio']].fillna(0)\n",
    "except Exception as e:\n",
    "    print(f\"Error computing amount variance/skewness: {e}\")\n",
    "    var_skew_df = pd.DataFrame({'entity_id': entities['entity_id'], 'avg_amount': 0, 'amount_variance': 0, 'amount_skewness': 0, 'high_value_ratio': 0})\n",
    "\n",
    "# Query 7: Transaction velocity, burstiness, and temporal concentration\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t:Transaction]->()\n",
    "        RETURN e.entity_id, t.timestamp AS timestamp\n",
    "    \"\"\")\n",
    "    time_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                          columns=['entity_id', 'timestamp'])\n",
    "    time_df['timestamp'] = pd.to_numeric(time_df['timestamp'], errors='coerce')\n",
    "    time_agg_df = time_df.groupby('entity_id').agg({\n",
    "        'timestamp': [\n",
    "            ('tx_velocity', lambda x: len(x) / (x.max() - x.min() + 1e-7)),\n",
    "            ('burstiness', lambda x: np.var(x, ddof=1) * 10 if len(x) > 1 else 0),\n",
    "            ('temporal_concentration', lambda x: np.mean((x.max() - x.min()) < 3) if len(x) > 0 else 0)\n",
    "        ]\n",
    "    }).droplevel(0, axis=1).reset_index()\n",
    "    time_agg_df[['tx_velocity', 'burstiness', 'temporal_concentration']] = time_agg_df[['tx_velocity', 'burstiness', 'temporal_concentration']].fillna(0)\n",
    "except Exception as e:\n",
    "    print(f\"Error computing temporal features: {e}\")\n",
    "    time_agg_df = pd.DataFrame({'entity_id': entities['entity_id'], 'tx_velocity': 0, 'burstiness': 0, 'temporal_concentration': 0})\n",
    "\n",
    "# Query 8: Additional features\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        OPTIONAL MATCH (e)-[t:Transaction]->()\n",
    "        OPTIONAL MATCH ()-[t_in:Transaction]->(e)\n",
    "        RETURN e.entity_id, \n",
    "               e.kyc_risk_score AS kyc_risk_score,\n",
    "               e.dormancy_period AS dormancy_period,\n",
    "               COALESCE(SUM(CASE WHEN t.is_cross_border THEN t.amount ELSE 0 END), 0) AS cross_border_amount,\n",
    "               COALESCE(COUNT(t_in), 0) AS in_degree,\n",
    "               COALESCE(COUNT(t), 0) AS tx_frequency,\n",
    "               COALESCE(COUNT(t_in) / (COUNT(t_in) + COUNT(t) + 1e-7), 0) AS directionality_ratio\n",
    "    \"\"\")\n",
    "    features_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                              columns=['entity_id', 'kyc_risk_score', 'dormancy_period', 'cross_border_amount', 'in_degree', 'tx_frequency', 'directionality_ratio'])\n",
    "    numeric_cols = ['kyc_risk_score', 'dormancy_period', 'cross_border_amount', 'in_degree', 'tx_frequency', 'directionality_ratio']\n",
    "    features_df[numeric_cols] = features_df[numeric_cols].apply(pd.to_numeric, errors='coerce', downcast='float').fillna(0)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing features query: {e}\")\n",
    "    features_df = pd.DataFrame({'entity_id': entities['entity_id'], 'kyc_risk_score': 0, 'dormancy_period': 0, 'cross_border_amount': 0, 'in_degree': 0, 'tx_frequency': 0, 'directionality_ratio': 0})\n",
    "\n",
    "# Query 9: Round-tripping (multi-hop)\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t1:Transaction]->(m:Entity)-[t2:Transaction]->(e2:Entity)\n",
    "        WHERE e2.entity_id = e.entity_id AND m.entity_id <> e.entity_id\n",
    "        RETURN e.entity_id, COUNT(DISTINCT t1) AS round_trip_count\n",
    "    \"\"\")\n",
    "    round_trip_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                                columns=['entity_id', 'round_trip_count'])\n",
    "    round_trip_df['round_trip_count'] = pd.to_numeric(round_trip_df['round_trip_count'], errors='coerce').fillna(0)\n",
    "except Exception as e:\n",
    "    print(f\"Error computing round-trip count: {e}\")\n",
    "    round_trip_df = pd.DataFrame({'entity_id': entities['entity_id'], 'round_trip_count': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Prep\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Merge features\n",
    "data_df = degree_df.merge(small_tx_df, on='entity_id', how='left').merge(var_skew_df, on='entity_id', how='left').merge(cluster_df, on='entity_id', how='left').merge(tx_freq_df, on='entity_id', how='left').merge(time_agg_df, on='entity_id', how='left').merge(features_df, on='entity_id', how='left').merge(round_trip_df, on='entity_id', how='left')\n",
    "data_df.fillna({'small_tx_count': 0, 'avg_amount': 0, 'amount_variance': 0, 'amount_skewness': 0, 'clustering_coeff': 0, 'tx_freq_variance': 0, 'tx_velocity': 0, 'burstiness': 0, 'temporal_concentration': 0, 'kyc_risk_score': 0, 'dormancy_period': 0, 'cross_border_amount': 0, 'in_degree': 0, 'tx_frequency': 0, 'directionality_ratio': 0, 'round_trip_count': 0}, inplace=True)\n",
    "print(\"Merged data_df:\")\n",
    "print(data_df.head())\n",
    "print(\"NaN counts:\")\n",
    "print(data_df.isna().sum())\n",
    "print(\"Data types:\")\n",
    "print(data_df.dtypes)\n",
    "\n",
    "# Compute ml_flag and labels\n",
    "try:\n",
    "    result = conn.execute(\"\"\"\n",
    "        MATCH (e:Entity)-[t:Transaction]->()\n",
    "        RETURN e.entity_id, AVG(t.ml_flag) AS ml_flag_score\n",
    "    \"\"\")\n",
    "    ml_flag_df = pd.DataFrame([result.get_next() for _ in range(result.get_num_tuples())], \n",
    "                             columns=['entity_id', 'ml_flag_score'])\n",
    "    ml_flag_df['ml_flag_score'] = pd.to_numeric(ml_flag_df['ml_flag_score'], errors='coerce').fillna(0)\n",
    "    data_df = data_df.merge(ml_flag_df, on='entity_id', how='left').fillna({'ml_flag_score': 0})\n",
    "    # Set initial is_anomaly based on non-zero ml_flag_score\n",
    "    data_df['is_anomaly'] = (data_df['ml_flag_score'] > 0).astype(int)\n",
    "    # Use ROC curve to optimize threshold if there are enough positive samples\n",
    "    if data_df['is_anomaly'].sum() > 1:  # Ensure enough positives for ROC\n",
    "        fpr, tpr, thresholds = roc_curve(data_df['is_anomaly'], data_df['ml_flag_score'])\n",
    "        anomaly_threshold = thresholds[np.argmax(tpr - fpr)]  # Youden’s J\n",
    "        data_df['is_anomaly'] = (data_df['ml_flag_score'] > anomaly_threshold).astype(int)\n",
    "    else:\n",
    "        print(\"Warning: Too few positive samples for ROC; using 95th percentile threshold\")\n",
    "        anomaly_threshold = np.percentile(data_df['ml_flag_score'], 95)\n",
    "        data_df['is_anomaly'] = (data_df['ml_flag_score'] > anomaly_threshold).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error computing ml_flag_score: {e}\")\n",
    "    data_df['ml_flag_score'] = np.zeros(len(data_df), dtype=float)\n",
    "    data_df['is_anomaly'] = np.zeros(len(data_df), dtype=int)\n",
    "\n",
    "# Feature selection\n",
    "feature_columns = ['degree', 'tx_velocity', 'kyc_risk_score', 'cross_border_amount', 'in_degree', 'directionality_ratio', 'round_trip_count']\n",
    "X = data_df[feature_columns].values\n",
    "\n",
    "# Verify X\n",
    "if not np.all(np.isfinite(X)):\n",
    "    print(\"Warning: X contains non-finite values\")\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(\"Any NaN in X:\", np.any(np.isnan(X)))\n",
    "print(\"Any inf in X:\", np.any(np.isinf(X)))\n",
    "\n",
    "# Split for held-out test set\n",
    "np.random.seed(42)\n",
    "test_size = 0.2\n",
    "test_idx = np.random.choice(len(X), size=int(test_size * len(X)), replace=False)\n",
    "train_idx = np.setdiff1d(np.arange(len(X)), test_idx)\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_test = data_df['is_anomaly'].iloc[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Train\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Autoencoder training on normal data\n",
    "normal_idx = np.random.choice(np.where(data_df.iloc[train_idx]['is_anomaly'] == 0)[0], size=int(0.5 * sum(data_df.iloc[train_idx]['is_anomaly'] == 0)), replace=False)\n",
    "X_normal = X_train[normal_idx]\n",
    "X_normal_tensor = torch.FloatTensor(X_normal)\n",
    "input_dim = X.shape[1]\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "# Training with early stopping\n",
    "X_val = X_train[np.random.choice(np.where(data_df.iloc[train_idx]['is_anomaly'] == 0)[0], size=int(0.1 * len(X_train)), replace=False)]\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "best_loss = float('inf')\n",
    "patience, max_patience = 0, 10\n",
    "for epoch in range(300):  # Increased epochs\n",
    "    autoencoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = autoencoder(X_normal_tensor)\n",
    "    loss = criterion(outputs, X_normal_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = autoencoder(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, X_val_tensor)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= max_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Compute reconstruction errors\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    reconstructed = autoencoder(X_tensor)\n",
    "    reconstruction_errors = torch.mean((reconstructed - X_tensor) ** 2, dim=1).numpy()\n",
    "anomaly_scores = 2000 * (reconstruction_errors - reconstruction_errors.min()) / (reconstruction_errors.max() - reconstruction_errors.min() + 1e-7)\n",
    "data_df['anomaly_score'] = anomaly_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, anomaly_scores[test_idx])\n",
    "optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "data_df['predicted_anomaly'] = (anomaly_scores > optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate on test set\n",
    "if 'is_anomaly' in data_df.columns:\n",
    "    precision = precision_score(y_test, data_df.iloc[test_idx]['predicted_anomaly'], zero_division=0)\n",
    "    recall = recall_score(y_test, data_df.iloc[test_idx]['predicted_anomaly'], zero_division=0)\n",
    "    f1 = f1_score(y_test, data_df.iloc[test_idx]['predicted_anomaly'], zero_division=0)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, anomaly_scores[test_idx])\n",
    "    auprc = auc(recall_curve, precision_curve)\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    print(f\"Test AUPRC: {auprc:.4f}\")\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Reduced to 5 folds\n",
    "fold = 1\n",
    "precisions, recalls, f1_scores, auprcs = [], [], [], []\n",
    "mean_anomaly_scores = []\n",
    "\n",
    "for train_idx_fold, val_idx in kf.split(X_train, data_df.iloc[train_idx]['is_anomaly']):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    X_train_fold, X_val = X_train[train_idx_fold], X_train[val_idx]\n",
    "    y_val = data_df['is_anomaly'].iloc[train_idx[val_idx]]\n",
    "    train_normal_idx = np.where(data_df.iloc[train_idx[train_idx_fold]]['is_anomaly'] == 0)[0]\n",
    "    X_train_normal = X_train_fold[train_normal_idx]\n",
    "    X_train_normal_tensor = torch.FloatTensor(X_train_normal)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    \n",
    "    autoencoder = Autoencoder(input_dim)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    for epoch in range(300):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(X_train_normal_tensor)\n",
    "        loss = criterion(outputs, X_train_normal_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = autoencoder(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, X_val_tensor)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_val = autoencoder(X_val_tensor)\n",
    "        val_errors = torch.mean((reconstructed_val - X_val_tensor) ** 2, dim=1).numpy()\n",
    "    val_scores = 2000 * (val_errors - val_errors.min()) / (val_errors.max() - val_errors.min() + 1e-7)\n",
    "    if y_val.sum() > 0:  # Check for positive samples\n",
    "        fpr, tpr, thresholds = roc_curve(y_val, val_scores)\n",
    "        optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    else:\n",
    "        print(f\"Warning: No positive samples in Fold {fold}; using 95th percentile threshold\")\n",
    "        optimal_threshold = np.percentile(val_scores, 95)\n",
    "    val_predictions = (val_scores > optimal_threshold).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_val, val_predictions, zero_division=0)\n",
    "    recall = recall_score(y_val, val_predictions, zero_division=0)\n",
    "    f1 = f1_score(y_val, val_predictions, zero_division=0)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_val, val_scores)\n",
    "    auprc = auc(recall_curve, precision_curve)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    auprcs.append(auprc)\n",
    "    mean_anomaly_scores.append(np.mean(val_scores))\n",
    "    \n",
    "    print(f\"Fold {fold} Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}\")\n",
    "    print(f\"Fold {fold} Mean Anomaly Score: {np.mean(val_scores):.4f}\")\n",
    "    print(f\"Validation Anomaly Scores (first 10): {val_scores[:10]}\")\n",
    "    fold += 1\n",
    "    \n",
    "# Average metrics\n",
    "print(f\"\\nAverage Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Average AUPRC: {np.mean(auprcs):.4f}\")\n",
    "print(f\"Average Mean Anomaly Score: {np.mean(mean_anomaly_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot anomaly score distribution with y-axis clipping\n",
    "plt.figure(figsize=(8, 5))\n",
    "scores = data_df['anomaly_score']\n",
    "anomaly_scores = data_df[data_df['is_anomaly'] == 1]['anomaly_score']\n",
    "hist, bins = np.histogram(scores, bins=30)  # Increased bins for finer detail\n",
    "hist_anomaly, _ = np.histogram(anomaly_scores, bins=bins)\n",
    "y_max = np.percentile(hist, 95)  # Clip y-axis at 95th percentile\n",
    "plt.hist(scores, bins=30, alpha=0.5, label='All Scores')\n",
    "plt.hist(anomaly_scores, bins=30, alpha=0.5, label='True Anomalies')\n",
    "plt.ylim(0, y_max)\n",
    "plt.title('Anomaly Score Distribution (Y-Axis Clipped at 95th Percentile)')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "fpr, tpr, _ = roc_curve(data_df['is_anomaly'], data_df['anomaly_score'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlations\n",
    "print(\"Feature correlations with anomaly score:\")\n",
    "correlations = data_df[feature_columns + ['anomaly_score']].corr()['anomaly_score'].drop('anomaly_score')\n",
    "print(correlations)\n",
    "\n",
    "# Save results\n",
    "data_df.to_csv('anomaly_results.csv', index=False)\n",
    "\n",
    "# Check for data leakage\n",
    "print(\"Feature means by anomaly label:\")\n",
    "print(data_df.groupby('is_anomaly')[['cross_border_amount', 'kyc_risk_score', 'dormancy_period']].mean())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
